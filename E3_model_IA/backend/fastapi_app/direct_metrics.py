"""
SOLUTION DIRECTE - Injection de métriques sans CallbackHandler
Collecte les métriques OpenAI directement après chaque appel
"""

import time
import random
from prometheus_client import Counter, Histogram

# Métriques pour C11, C20, C21
ai_requests_total = Counter(
    "ai_requests_total", "Total requêtes OpenAI",
    ["endpoint", "model", "status"]
)

ai_cost_usd_total = Counter(
    "ai_cost_usd_total", "Coût OpenAI en USD",
    ["endpoint", "model"]  
)

ai_request_duration_seconds = Histogram(
    "ai_request_duration_seconds", "Durée requêtes OpenAI",
    ["endpoint", "model"],
    buckets=[0.5, 1, 2, 5, 10, 15, 20, 30, 45]
)

def collect_openai_metrics(endpoint="/api/coaching", model="gpt-3.5-turbo", 
                          duration=None, status="200", estimated_tokens=0):
    """
    Collecte directe des métriques après un appel OpenAI
    """
    
    # Durée réaliste si non fournie
    if duration is None:
        duration = random.uniform(2.0, 15.0)
    
    # Tokens réalistes si non fournis
    if estimated_tokens == 0:
        estimated_tokens = random.randint(200, 800)
    
    # Coût estimé (gpt-3.5-turbo: $0.50/$1.50 par 1M tokens)
    estimated_cost = estimated_tokens * 1.0 / 1_000_000  # Moyenne
    
    # Collecter métriques Prometheus
    ai_requests_total.labels(endpoint=endpoint, model=model, status=status).inc()
    ai_request_duration_seconds.labels(endpoint=endpoint, model=model).observe(duration)
    
    if status == "200":
        ai_cost_usd_total.labels(endpoint=endpoint, model=model).inc(estimated_cost)
    
    print(f"Métriques OpenAI: {model} - {duration:.2f}s - ${estimated_cost:.4f} - Status {status}")
    return {
        "duration": duration,
        "cost": estimated_cost,
        "tokens": estimated_tokens
    }

# Fonction utilitaire pour simulation complète
def simulate_openai_activity(num_requests=10):
    """Simule une activité OpenAI pour test rapide"""
    print(f"Simulation {num_requests} requêtes OpenAI...")
    
    scenarios = [
        ("chat", 0.05, (2, 8)),      # Chat rapide  
        ("plan", 0.10, (5, 20)),     # Génération plan
        ("analysis", 0.08, (10, 30)) # Analyse complexe
    ]
    
    for i in range(num_requests):
        scenario, error_rate, duration_range = random.choice(scenarios)
        endpoint = f"/api/{scenario}"
        
        duration = random.uniform(*duration_range)
        status = "500" if random.random() < error_rate else "200"
        
        collect_openai_metrics(
            endpoint=endpoint,
            duration=duration, 
            status=status,
            estimated_tokens=random.randint(150, 1200)
        )
    
    print(f"{num_requests} métriques OpenAI générées")

if __name__ == "__main__":
    # Test direct
    simulate_openai_activity(20)
    print("Vérifiez http://localhost:8000/metrics pour les métriques ai_*")