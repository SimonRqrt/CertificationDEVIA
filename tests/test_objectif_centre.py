#!/usr/bin/env python3
"""
Script de test pour l'approche "objectif-centr√©e" de g√©n√©ration de plans d'entra√Ænement.

Ce script teste si l'agent IA d√©termine automatiquement une dur√©e optimale en fonction de l'objectif
plut√¥t que de laisser l'utilisateur choisir une dur√©e arbitraire.

Deux sc√©narios sont test√©s :
- Sc√©nario A : Avec target_time="45:00" (objectif de temps sp√©cifique)
- Sc√©nario B : Sans target_time (objectif g√©n√©ral)

L'agent doit analyser l'√©cart entre la performance actuelle et l'objectif pour d√©terminer
une dur√©e de pr√©paration appropri√©e (pas seulement 2 semaines).
"""

import asyncio
import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# Ajouter le chemin du projet pour les imports
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Charger les variables d'environnement
load_dotenv(project_root / ".env")

from rich import print as rprint
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from langchain_core.messages import HumanMessage

# Import de l'agent
from E3_model_IA.scripts.advanced_agent import get_coaching_graph

console = Console()

class ObjectiveCenteredTester:
    """Classe pour tester l'approche objectif-centr√©e"""
    
    def __init__(self):
        self.results = []
        self.test_start_time = datetime.now()
        
    async def initialize_agent(self):
        """Initialiser l'agent de coaching"""
        rprint("[yellow]üìã Initialisation de l'agent de coaching...[/yellow]")
        self.graph = await get_coaching_graph()
        rprint("[green]‚úÖ Agent initialis√© avec succ√®s[/green]")
    
    def create_prompt(self, scenario_name, target_time=None, duration_weeks=0):
        """Cr√©er le prompt selon l'approche objectif-centr√©e"""
        target_time_info = ""
        if target_time:
            target_time_info = f"- TEMPS OBJECTIF: {target_time} sur 10k"
        
        duration_instruction = ""
        if duration_weeks > 0:
            duration_instruction = f"- DUR√âE IMPOS√âE: {duration_weeks} semaines exactement"
        else:
            duration_instruction = "- DUR√âE √Ä D√âTERMINER: Analyse l'√©cart entre niveau actuel et objectif pour d√©terminer la dur√©e optimale (4-20 semaines)"
        
        prompt = f"""Je suis l'utilisateur 1.

ANALYSE ET G√âN√àRE un plan d'entra√Ænement intelligent:
- Objectif: 10k  
- Niveau d√©clar√©: interm√©diaire
- 3 s√©ances/semaine
{target_time_info}
{duration_instruction}

√âTAPES OBLIGATOIRES:
1. UTILISE get_user_metrics_from_db(1) pour analyser le niveau r√©el
2. √âVALUE l'√©cart entre performance actuelle et objectif cible
3. D√âTERMINE la dur√©e optimale de pr√©paration (justifie ton choix)
4. G√âN√àRE le plan COMPLET sur cette dur√©e avec tableaux markdown pour CHAQUE semaine

IMPORTANT: Si objectif de temps donn√©, calcule une progression r√©aliste. Sinon, utilise dur√©es standards selon l'objectif.

Sois pr√©cis, r√©aliste et justifie tes choix."""

        return prompt
    
    async def run_scenario(self, scenario_name, description, target_time=None, duration_weeks=0):
        """Ex√©cuter un sc√©nario de test"""
        rprint(f"\n[bold cyan]üß™ {scenario_name}: {description}[/bold cyan]")
        rprint("=" * 80)
        
        # Pr√©parer le prompt
        prompt = self.create_prompt(scenario_name, target_time, duration_weeks)
        
        rprint(f"[yellow]üì§ Prompt envoy√© √† l'agent :[/yellow]")
        rprint(Panel(prompt, title="Prompt", border_style="yellow"))
        
        # Param√®tres du test
        user_id = 1
        thread_id = f"test-objectif-{scenario_name.lower().replace(' ', '-')}-{int(time.time())}"
        
        rprint(f"[yellow]‚ö° Ex√©cution du {scenario_name}...[/yellow]")
        rprint("(Cela peut prendre 30-60 secondes)")
        
        start_time = time.time()
        full_response = ""
        
        try:
            # Configurer le mode plan_generator pour forcer la g√©n√©ration multi-semaines
            initial_state = {
                "messages": [HumanMessage(content=prompt)],
                "mode": "plan_generator"
            }
            
            # Stream de l'ex√©cution
            async for event in self.graph.astream(
                initial_state,
                config={"configurable": {"thread_id": thread_id}}
            ):
                for step in event.values():
                    if "messages" in step and step["messages"]:
                        message = step["messages"][-1]
                        if hasattr(message, 'content') and message.content:
                            if hasattr(message, 'type') and message.type == "ai":
                                full_response += message.content
            
            execution_time = time.time() - start_time
            
            rprint(f"[green]‚úÖ {scenario_name} termin√© en {execution_time:.1f}s[/green]")
            
            # Analyser la r√©ponse
            analysis = self.analyze_response(full_response, target_time, duration_weeks)
            
            # Stocker les r√©sultats
            result = {
                "scenario": scenario_name,
                "description": description,
                "target_time": target_time,
                "duration_weeks_input": duration_weeks,
                "prompt": prompt,
                "response": full_response,
                "execution_time": execution_time,
                "analysis": analysis,
                "timestamp": datetime.now().isoformat()
            }
            self.results.append(result)
            
            # Afficher le r√©sum√©
            self.display_scenario_summary(result)
            
            return result
            
        except Exception as e:
            rprint(f"[bold red]‚ùå Erreur lors du {scenario_name}: {e}[/bold red]")
            import traceback
            traceback.print_exc()
            return None
    
    def analyze_response(self, response, target_time, duration_weeks_input):
        """Analyser la r√©ponse de l'agent"""
        analysis = {
            "response_length": len(response),
            "weeks_found": [],
            "duration_determined": None,
            "justification_found": False,
            "gap_analysis_found": False,
            "complete_plan_found": False,
            "metrics_used": False,
            "target_time_mentioned": False,
            "progression_calculated": False
        }
        
        # Rechercher les semaines num√©rot√©es
        for i in range(1, 25):  # Chercher jusqu'√† 24 semaines
            week_patterns = [
                f"semaine {i}",
                f"## semaine {i}",
                f"# semaine {i}",
                f"### semaine {i}"
            ]
            for pattern in week_patterns:
                if pattern in response.lower():
                    analysis["weeks_found"].append(i)
                    break
        
        # D√©terminer la dur√©e effective
        max_week = max(analysis["weeks_found"]) if analysis["weeks_found"] else 0
        analysis["duration_determined"] = max_week
        
        # Rechercher des justifications de dur√©e
        justification_keywords = [
            "semaines n√©cessaires",
            "dur√©e optimale",
            "progression progressive",
            "temps de pr√©paration",
            "√©cart entre",
            "justification",
            "raison de cette dur√©e",
            "choix de la dur√©e"
        ]
        
        for keyword in justification_keywords:
            if keyword.lower() in response.lower():
                analysis["justification_found"] = True
                break
        
        # Rechercher l'analyse de l'√©cart
        gap_keywords = [
            "√©cart entre",
            "niveau actuel",
            "objectif cible",
            "performance actuelle",
            "progresser de",
            "am√©lioration n√©cessaire"
        ]
        
        for keyword in gap_keywords:
            if keyword.lower() in response.lower():
                analysis["gap_analysis_found"] = True
                break
        
        # V√©rifier la structure de plan complet
        plan_indicators = [
            "| Jour" in response and "| Type S√©ance" in response,
            response.count("##") >= analysis["duration_determined"],  # Un titre par semaine minimum
            "tableau" in response.lower() or "plan" in response.lower()
        ]
        analysis["complete_plan_found"] = any(plan_indicators)
        
        # V√©rifier l'utilisation des m√©triques
        metrics_keywords = [
            "get_user_metrics_from_db",
            "m√©triques utilisateur",
            "donn√©es utilisateur",
            "profil utilisateur",
            "activit√©s r√©centes"
        ]
        
        for keyword in metrics_keywords:
            if keyword.lower() in response.lower():
                analysis["metrics_used"] = True
                break
        
        # V√©rifier la mention du target_time
        if target_time:
            analysis["target_time_mentioned"] = target_time in response or "45:00" in response or "45 min" in response
        
        # V√©rifier le calcul de progression
        progression_keywords = [
            "progression",
            "r√©aliste",
            "objectif atteignable",
            "am√©lioration graduelle",
            "paliers"
        ]
        
        for keyword in progression_keywords:
            if keyword.lower() in response.lower():
                analysis["progression_calculated"] = True
                break
        
        return analysis
    
    def display_scenario_summary(self, result):
        """Afficher le r√©sum√© d'un sc√©nario"""
        analysis = result["analysis"]
        
        # Cr√©er un tableau de r√©sum√©
        table = Table(title=f"R√©sum√© - {result['scenario']}")
        table.add_column("Crit√®re", style="cyan")
        table.add_column("R√©sultat", style="green")
        table.add_column("Statut", justify="center")
        
        # Dur√©e d√©termin√©e
        duration_status = "‚úÖ" if analysis["duration_determined"] > 2 else "‚ùå"
        table.add_row(
            "Dur√©e d√©termin√©e", 
            f"{analysis['duration_determined']} semaines",
            duration_status
        )
        
        # Justification
        justification_status = "‚úÖ" if analysis["justification_found"] else "‚ùå"
        table.add_row(
            "Justification de dur√©e", 
            "Trouv√©e" if analysis["justification_found"] else "Manquante",
            justification_status
        )
        
        # Analyse d'√©cart
        gap_status = "‚úÖ" if analysis["gap_analysis_found"] else "‚ùå"
        table.add_row(
            "Analyse d'√©cart", 
            "Effectu√©e" if analysis["gap_analysis_found"] else "Manquante",
            gap_status
        )
        
        # Utilisation des m√©triques
        metrics_status = "‚úÖ" if analysis["metrics_used"] else "‚ùå"
        table.add_row(
            "Utilisation m√©triques", 
            "Oui" if analysis["metrics_used"] else "Non",
            metrics_status
        )
        
        # Plan complet
        plan_status = "‚úÖ" if analysis["complete_plan_found"] else "‚ùå"
        table.add_row(
            "Plan complet g√©n√©r√©", 
            "Oui" if analysis["complete_plan_found"] else "Non",
            plan_status
        )
        
        # Target time (si applicable)
        if result["target_time"]:
            target_time_status = "‚úÖ" if analysis["target_time_mentioned"] else "‚ùå"
            table.add_row(
                "Objectif de temps pris en compte", 
                "Oui" if analysis["target_time_mentioned"] else "Non",
                target_time_status
            )
        
        console.print(table)
        
        # Afficher un extrait de la r√©ponse
        response_preview = result["response"][:500] + "..." if len(result["response"]) > 500 else result["response"]
        rprint(f"\n[yellow]üìñ Extrait de la r√©ponse :[/yellow]")
        rprint(Panel(response_preview, title="R√©ponse (extrait)", border_style="blue"))
    
    def generate_detailed_report(self):
        """G√©n√©rer le rapport d√©taill√© final"""
        report_content = self.create_report_content()
        
        # Sauvegarder dans un fichier
        timestamp = self.test_start_time.strftime("%Y%m%d_%H%M%S")
        report_filename = f"rapport_objectif_centre_{timestamp}.md"
        
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        rprint(f"\n[bold green]üìä Rapport d√©taill√© g√©n√©r√©: {report_filename}[/bold green]")
        
        # Sauvegarder les donn√©es brutes en JSON
        json_filename = f"donnees_test_objectif_centre_{timestamp}.json"
        with open(json_filename, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        
        rprint(f"[cyan]üìÑ Donn√©es brutes sauvegard√©es: {json_filename}[/cyan]")
        
        return report_filename
    
    def create_report_content(self):
        """Cr√©er le contenu du rapport d√©taill√©"""
        timestamp = self.test_start_time.strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""# Rapport de Test : Approche "Objectif-Centr√©e"

**Date et heure du test :** {timestamp}
**Objectif :** Valider que l'agent IA d√©termine automatiquement une dur√©e optimale en fonction de l'objectif plut√¥t que de laisser l'utilisateur choisir une dur√©e arbitraire.

## R√©sum√© Ex√©cutif

Ce test compare l'efficacit√© de la nouvelle approche "objectif-centr√©e" par rapport √† l'ancienne approche "dur√©e-centr√©e". L'agent doit analyser l'√©cart entre la performance actuelle de l'utilisateur et son objectif pour d√©terminer une dur√©e de pr√©paration appropri√©e.

## Sc√©narios Test√©s

"""
        
        for i, result in enumerate(self.results, 1):
            analysis = result["analysis"]
            
            report += f"""### Sc√©nario {i}: {result['scenario']}

**Description :** {result['description']}

**Param√®tres d'entr√©e :**
- Objectif : 10k
- Niveau : interm√©diaire  
- Fr√©quence : 3 s√©ances/semaine
- Target time : {result['target_time'] if result['target_time'] else 'Non sp√©cifi√©'}
- Duration weeks : {result['duration_weeks_input']} (0 = agent d√©termine)

**Prompt envoy√© :**
```
{result['prompt']}
```

**Temps d'ex√©cution :** {result['execution_time']:.1f} secondes

**R√©sultats de l'analyse :**

| Crit√®re | R√©sultat | Statut |
|---------|----------|--------|
| Dur√©e d√©termin√©e | {analysis['duration_determined']} semaines | {'‚úÖ Succ√®s' if analysis['duration_determined'] > 2 else '‚ùå √âchec'} |
| Justification fournie | {'Oui' if analysis['justification_found'] else 'Non'} | {'‚úÖ' if analysis['justification_found'] else '‚ùå'} |
| Analyse d'√©cart effectu√©e | {'Oui' if analysis['gap_analysis_found'] else 'Non'} | {'‚úÖ' if analysis['gap_analysis_found'] else '‚ùå'} |
| M√©triques utilisateur utilis√©es | {'Oui' if analysis['metrics_used'] else 'Non'} | {'‚úÖ' if analysis['metrics_used'] else '‚ùå'} |
| Plan complet g√©n√©r√© | {'Oui' if analysis['complete_plan_found'] else 'Non'} | {'‚úÖ' if analysis['complete_plan_found'] else '‚ùå'} |
| Objectif temps pris en compte | {'Oui' if analysis.get('target_time_mentioned', False) else 'Non applicable' if not result['target_time'] else 'Non'} | {'‚úÖ' if analysis.get('target_time_mentioned', True) else '‚ùå'} |

**Semaines d√©tect√©es dans le plan :** {', '.join(map(str, analysis['weeks_found'][:10]))}{'...' if len(analysis['weeks_found']) > 10 else ''}

**R√©ponse compl√®te de l'agent :**

```markdown
{result['response']}
```

---

"""
        
        # Analyse comparative
        report += """## Analyse Comparative

"""
        
        if len(self.results) >= 2:
            scenario_a = self.results[0]['analysis']
            scenario_b = self.results[1]['analysis']
            
            report += f"""### Comparaison des Dur√©es D√©termin√©es

- **Sc√©nario A (avec target_time)** : {scenario_a['duration_determined']} semaines
- **Sc√©nario B (sans target_time)** : {scenario_b['duration_determined']} semaines

### Qualit√© des Justifications

- **Sc√©nario A** : {'Justification trouv√©e' if scenario_a['justification_found'] else 'Justification manquante'}
- **Sc√©nario B** : {'Justification trouv√©e' if scenario_b['justification_found'] else 'Justification manquante'}

### Utilisation des M√©triques Utilisateur

- **Sc√©nario A** : {'M√©triques utilis√©es' if scenario_a['metrics_used'] else 'M√©triques non utilis√©es'}
- **Sc√©nario B** : {'M√©triques utilis√©es' if scenario_b['metrics_used'] else 'M√©triques non utilis√©es'}

"""
        
        # √âvaluation globale
        report += """## √âvaluation Globale

### Points Positifs

"""
        
        positive_points = []
        negative_points = []
        
        for result in self.results:
            analysis = result['analysis']
            scenario = result['scenario']
            
            if analysis['duration_determined'] > 2:
                positive_points.append(f"‚úÖ {scenario}: L'agent a d√©termin√© une dur√©e r√©aliste de {analysis['duration_determined']} semaines (>2)")
            else:
                negative_points.append(f"‚ùå {scenario}: Dur√©e insuffisante de {analysis['duration_determined']} semaines")
                
            if analysis['justification_found']:
                positive_points.append(f"‚úÖ {scenario}: Justification de la dur√©e choisie fournie")
            else:
                negative_points.append(f"‚ùå {scenario}: Absence de justification pour la dur√©e choisie")
                
            if analysis['gap_analysis_found']:
                positive_points.append(f"‚úÖ {scenario}: Analyse de l'√©cart entre niveau actuel et objectif")
            else:
                negative_points.append(f"‚ùå {scenario}: Pas d'analyse d'√©cart d√©tect√©e")
        
        for point in positive_points[:10]:  # Limiter √† 10 points max
            report += f"- {point}\n"
        
        report += """
### Points d'Am√©lioration

"""
        
        for point in negative_points[:10]:  # Limiter √† 10 points max
            report += f"- {point}\n"
        
        # Recommandations
        report += """
## Recommandations

### Am√©liorations du Prompt

"""
        
        if not all(r['analysis']['justification_found'] for r in self.results):
            report += """- **Renforcer l'exigence de justification** : Ajouter "OBLIGATOIRE: Justifie explicitement pourquoi tu choisis cette dur√©e" dans le prompt.
"""
        
        if not all(r['analysis']['gap_analysis_found'] for r in self.results):
            report += """- **Clarifier l'analyse d'√©cart** : Reformuler l'√©tape 2 pour √™tre plus explicite : "CALCULE l'√©cart en pourcentage entre la performance actuelle et l'objectif cible".
"""
        
        if not all(r['analysis']['metrics_used'] for r in self.results):
            report += """- **Forcer l'utilisation des m√©triques** : Ajouter une v√©rification que l'agent a bien appel√© get_user_metrics_from_db avant de continuer.
"""
        
        report += """
### Am√©liorations Techniques

- **Validation des r√©ponses** : Impl√©menter une v√©rification post-g√©n√©ration pour s'assurer que le nombre de semaines g√©n√©r√©es correspond √† la dur√©e d√©termin√©e.
- **M√©triques de performance** : Ajouter des m√©triques pour mesurer la coh√©rence entre l'objectif, le niveau utilisateur et la dur√©e propos√©e.
- **Feedback loop** : Impl√©menter un syst√®me de retour utilisateur pour affiner les dur√©es propos√©es.

## Conclusion

"""
        
        success_rate = sum(1 for r in self.results if r['analysis']['duration_determined'] > 2) / len(self.results) * 100
        
        if success_rate >= 80:
            conclusion_status = "SUCC√àS"
            conclusion_color = "‚úÖ"
        elif success_rate >= 50:
            conclusion_status = "MITIG√â"  
            conclusion_color = "‚ö†Ô∏è"
        else:
            conclusion_status = "√âCHEC"
            conclusion_color = "‚ùå"
        
        report += f"""{conclusion_color} **Statut global : {conclusion_status}** ({success_rate:.0f}% de r√©ussite)

L'approche "objectif-centr√©e" {"d√©montre son efficacit√©" if success_rate >= 80 else "n√©cessite des am√©liorations" if success_rate >= 50 else "pr√©sente des d√©faillances importantes"} dans la d√©termination automatique de dur√©es de pr√©paration appropri√©es.

{"Les agents analysent correctement les objectifs et g√©n√®rent des plans de dur√©e r√©aliste." if success_rate >= 80 else "Des am√©liorations sont n√©cessaires pour garantir la coh√©rence des dur√©es propos√©es." if success_rate >= 50 else "Une r√©vision majeure du syst√®me de d√©termination de dur√©e est recommand√©e."}

---

*Rapport g√©n√©r√© automatiquement le {timestamp}*
"""
        
        return report

async def main():
    """Fonction principale du test"""
    console.rule("[bold blue]Test de l'Approche Objectif-Centr√©e[/bold blue]", style="blue")
    
    tester = ObjectiveCenteredTester()
    
    try:
        # Initialiser l'agent
        await tester.initialize_agent()
        
        # Sc√©nario A : Avec target_time="45:00" (objectif de temps sp√©cifique)
        await tester.run_scenario(
            scenario_name="Sc√©nario A",
            description="Objectif de temps sp√©cifique - 10k en 45:00",
            target_time="45:00",
            duration_weeks=0  # Agent d√©termine automatiquement
        )
        
        # Pause entre les sc√©narios
        rprint("\n[yellow]‚è≥ Pause de 5 secondes entre les sc√©narios...[/yellow]")
        await asyncio.sleep(5)
        
        # Sc√©nario B : Sans target_time (objectif g√©n√©ral)
        await tester.run_scenario(
            scenario_name="Sc√©nario B", 
            description="Objectif g√©n√©ral - 10k sans temps sp√©cifique",
            target_time=None,
            duration_weeks=0  # Agent d√©termine automatiquement
        )
        
        # G√©n√©rer le rapport d√©taill√©
        rprint("\n[bold cyan]üìä G√©n√©ration du rapport d√©taill√©...[/bold cyan]")
        report_file = tester.generate_detailed_report()
        
        # R√©sum√© final
        console.rule("[bold green]Test Termin√©[/bold green]", style="green")
        
        success_count = sum(1 for r in tester.results if r['analysis']['duration_determined'] > 2)
        total_scenarios = len(tester.results)
        
        rprint(f"\n[bold]R√©sultats finaux :[/bold]")
        rprint(f"   - Sc√©narios r√©ussis : {success_count}/{total_scenarios}")
        rprint(f"   - Rapport d√©taill√© : {report_file}")
        
        if success_count == total_scenarios:
            rprint("[bold green]üéâ Tous les sc√©narios ont r√©ussi ! L'approche objectif-centr√©e fonctionne correctement.[/bold green]")
        elif success_count > 0:
            rprint("[bold yellow]‚ö†Ô∏è Certains sc√©narios n√©cessitent des am√©liorations.[/bold yellow]")
        else:
            rprint("[bold red]‚ùå L'approche objectif-centr√©e n√©cessite une r√©vision compl√®te.[/bold red]")
        
        return 0 if success_count == total_scenarios else 1
        
    except KeyboardInterrupt:
        rprint("\n[yellow]‚ö†Ô∏è Test interrompu par l'utilisateur[/yellow]")
        return 1
    except Exception as e:
        rprint(f"[bold red]üí• Erreur fatale: {e}[/bold red]")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(asyncio.run(main()))