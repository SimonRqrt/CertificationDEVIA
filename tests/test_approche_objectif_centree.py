"""
Script de test pour valider l'approche "objectif-centr√©e" 
de la g√©n√©ration de plans d'entra√Ænement

Contexte : 
L'agent IA d√©termine maintenant automatiquement la dur√©e optimale 
en fonction de l'objectif de l'utilisateur, plut√¥t que de laisser 
l'utilisateur choisir une dur√©e arbitraire.

Tests :
1. Sc√©nario A : Avec target_time="45:00" (objectif de temps pour 10k)
2. Sc√©nario B : Sans target_time="" (objectif g√©n√©ral pour 10k)
"""

import sys
import os
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path

# Ajouter le chemin du projet
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.append(project_root)

try:
    from E3_model_IA.scripts.advanced_agent import get_coaching_graph
    from langchain_core.messages import HumanMessage
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    print("Assurez-vous que les d√©pendances sont install√©es et que les chemins sont corrects")
    sys.exit(1)

class TestApprocheeObjectifCentree:
    """Tests pour valider l'approche objectif-centr√©e"""
    
    def setup_method(self):
        """Initialisation commune √† tous les tests"""
        self.results = {
            "test_timestamp": datetime.now().isoformat(),
            "scenarios": {},
            "analysis": {},
            "recommendations": []
        }
    
    async def setup_agent(self):
        """Initialisation de l'agent IA"""
        print("üîß Initialisation de l'agent IA...")
        try:
            self.graph = await get_coaching_graph()
            self.user_id = 1  # Utilisateur de test
            print("‚úÖ Agent IA initialis√© avec succ√®s")
            return True
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation : {e}")
            return False
    
    def create_test_prompt_scenario_a(self):
        """Sc√©nario A : Avec target_time pour objectif de temps"""
        return f"""
Je suis l'utilisateur {self.user_id}. Je souhaite g√©n√©rer un plan d'entra√Ænement avec ces param√®tres :

OBJECTIF : Courir un 10k en 45:00 (quarante-cinq minutes)
NIVEAU : interm√©diaire 
SESSIONS : 3 par semaine
DUR√âE : 0 semaines (laisse l'agent d√©terminer la dur√©e optimale)
TARGET_TIME : 45:00

Instructions pour l'agent :
1. Analyse d'abord mes donn√©es utilisateur avec get_user_metrics_from_db
2. Recherche les connaissances pertinentes avec get_training_knowledge  
3. D√©termine automatiquement la dur√©e optimale du plan (PAS seulement 2 semaines)
4. Justifie ton choix de dur√©e en fonction de l'√©cart entre ma performance actuelle et l'objectif
5. G√©n√®re un plan complet couvrant TOUTE la dur√©e choisie

L'objectif est d'am√©liorer ma vitesse pour atteindre 45:00 sur 10k. 
G√©n√®re un plan d'entra√Ænement personnalis√© sur plusieurs semaines.
"""
    
    def create_test_prompt_scenario_b(self):
        """Sc√©nario B : Sans target_time pour objectif g√©n√©ral"""
        return f"""
Je suis l'utilisateur {self.user_id}. Je souhaite g√©n√©rer un plan d'entra√Ænement avec ces param√®tres :

OBJECTIF : Am√©liorer ma performance sur 10k (objectif g√©n√©ral)
NIVEAU : interm√©diaire
SESSIONS : 3 par semaine  
DUR√âE : 0 semaines (laisse l'agent d√©terminer la dur√©e optimale)
TARGET_TIME : (aucun temps sp√©cifique)

Instructions pour l'agent :
1. Analyse d'abord mes donn√©es utilisateur avec get_user_metrics_from_db
2. Recherche les connaissances pertinentes avec get_training_knowledge
3. D√©termine automatiquement la dur√©e optimale du plan bas√©e sur mon niveau
4. Propose un objectif de temps r√©aliste bas√© sur mes performances actuelles
5. G√©n√®re un plan complet couvrant TOUTE la dur√©e choisie

L'objectif est d'am√©liorer ma performance g√©n√©rale sur 10k sans temps cible sp√©cifique.
G√©n√®re un plan d'entra√Ænement personnalis√© sur plusieurs semaines.
"""
    
    async def run_scenario(self, scenario_name, prompt, max_retries=2):
        """Ex√©cution d'un sc√©nario de test avec gestion des erreurs"""
        print(f"\n{'='*60}")
        print(f"üß™ EX√âCUTION - {scenario_name}")
        print(f"{'='*60}")
        
        scenario_result = {
            "prompt": prompt,
            "response": "",
            "analysis": {},
            "execution_time": 0,
            "errors": [],
            "retries": 0
        }
        
        for attempt in range(max_retries + 1):
            try:
                start_time = time.time()
                
                print(f"üì§ Envoi du prompt √† l'agent (tentative {attempt + 1})...")
                print(f"üìù Prompt : {prompt[:200]}...")
                
                # Configuration de la conversation
                config = {
                    "configurable": {
                        "thread_id": f"test-objective-{scenario_name.lower()}-{int(time.time())}"
                    }
                }
                
                response_parts = []
                async for event in self.graph.astream(
                    {"messages": [HumanMessage(content=prompt)], "mode": "plan_generator"},
                    config=config
                ):
                    for step in event.values():
                        if "messages" in step and step["messages"]:
                            message = step["messages"][-1]
                            if hasattr(message, 'content') and message.content:
                                response_parts.append(str(message.content))
                                print(f"üì• R√©ponse partielle: {str(message.content)[:100]}...")
                
                execution_time = time.time() - start_time
                full_response = '\n'.join(response_parts)
                
                if not full_response.strip():
                    raise Exception("R√©ponse vide de l'agent")
                
                scenario_result.update({
                    "response": full_response,
                    "execution_time": execution_time,
                    "retries": attempt
                })
                
                print(f"‚úÖ Sc√©nario {scenario_name} ex√©cut√© avec succ√®s en {execution_time:.2f}s")
                break
                
            except Exception as e:
                error_msg = f"Tentative {attempt + 1} √©chou√©e: {str(e)}"
                scenario_result["errors"].append(error_msg)
                print(f"‚ùå {error_msg}")
                
                if attempt < max_retries:
                    wait_time = 2 ** attempt  # Backoff exponentiel
                    print(f"‚è≥ Attente {wait_time}s avant nouvelle tentative...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"‚ùå Sc√©nario {scenario_name} √©chou√© apr√®s {max_retries + 1} tentatives")
                    scenario_result["response"] = f"√âCHEC: {'; '.join(scenario_result['errors'])}"
        
        return scenario_result
    
    def analyze_response(self, scenario_name, response):
        """Analyse de la r√©ponse pour validation"""
        analysis = {
            "duration_determined": False,
            "duration_weeks": 0,
            "duration_justified": False,
            "performance_gap_analyzed": False,
            "complete_plan_generated": False,
            "week_count": 0,
            "target_time_considered": False,
            "recommendations_quality": "unknown"
        }
        
        response_lower = response.lower()
        
        # 1. V√©rification que la dur√©e est d√©termin√©e
        duration_indicators = ["semaine", "weeks", "dur√©e", "plan de", "programme de"]
        for indicator in duration_indicators:
            if indicator in response_lower:
                analysis["duration_determined"] = True
                break
        
        # 2. Extraction du nombre de semaines
        import re
        week_patterns = [
            r'(\d+)\s*semaine',
            r'semaine\s*(\d+)',
            r'plan de\s*(\d+)',
            r'programme de\s*(\d+)',
            r'##\s*semaine\s*(\d+)'
        ]
        
        week_numbers = []
        for pattern in week_patterns:
            matches = re.findall(pattern, response_lower)
            week_numbers.extend([int(m) for m in matches])
        
        if week_numbers:
            analysis["duration_weeks"] = max(week_numbers)
            analysis["week_count"] = len(set(week_numbers))
        
        # 3. V√©rification que la dur√©e est justifi√©e
        justification_indicators = [
            "parce que", "car", "en raison", "afin de", "pour atteindre",
            "objectif", "performance", "progression", "am√©lioration"
        ]
        analysis["duration_justified"] = any(ind in response_lower for ind in justification_indicators)
        
        # 4. Analyse de l'√©cart de performance
        gap_indicators = [
            "√©cart", "diff√©rence", "am√©liorer", "progression", "actuel", 
            "objectif", "performance", "temps", "vitesse"
        ]
        analysis["performance_gap_analyzed"] = any(ind in response_lower for ind in gap_indicators)
        
        # 5. V√©rification du plan complet
        plan_indicators = ["tableau", "jour", "lundi", "mardi", "s√©ance", "entra√Ænement"]
        analysis["complete_plan_generated"] = any(ind in response_lower for ind in plan_indicators)
        
        # 6. V√©rification de la prise en compte du target_time
        time_indicators = ["45:00", "quarante-cinq", "45 minutes", "temps objectif"]
        analysis["target_time_considered"] = any(ind in response_lower for ind in time_indicators)
        
        # 7. Qualit√© des recommandations
        if analysis["duration_weeks"] >= 6 and analysis["complete_plan_generated"]:
            analysis["recommendations_quality"] = "excellent"
        elif analysis["duration_weeks"] >= 4 and analysis["duration_justified"]:
            analysis["recommendations_quality"] = "good"
        elif analysis["duration_determined"]:
            analysis["recommendations_quality"] = "basic"
        else:
            analysis["recommendations_quality"] = "poor"
        
        return analysis
    
    def generate_recommendations(self):
        """G√©n√©ration des recommandations d'am√©lioration"""
        recommendations = []
        
        # Analyse comparative des sc√©narios
        scenario_a = self.results["scenarios"].get("Sc√©nario A", {})
        scenario_b = self.results["scenarios"].get("Sc√©nario B", {})
        
        analysis_a = scenario_a.get("analysis", {})
        analysis_b = scenario_b.get("analysis", {})
        
        # Recommandations bas√©es sur la dur√©e
        if analysis_a.get("duration_weeks", 0) <= 2:
            recommendations.append(
                "üî¥ CRITIQUE: L'agent g√©n√®re des plans trop courts (‚â§2 semaines). "
                "Modifier le prompt pour insister sur des plans de 6-12 semaines minimum."
            )
        elif analysis_a.get("duration_weeks", 0) < 6:
            recommendations.append(
                "üü° AM√âLIORATION: L'agent pourrait g√©n√©rer des plans plus longs "
                "pour permettre une progression graduelle et durable."
            )
        else:
            recommendations.append(
                "üü¢ EXCELLENT: L'agent g√©n√®re des plans de dur√©e appropri√©e "
                f"({analysis_a.get('duration_weeks', 0)} semaines)."
            )
        
        # Recommandations sur la justification
        if not analysis_a.get("duration_justified"):
            recommendations.append(
                "üî¥ MANQUANT: L'agent ne justifie pas suffisamment son choix de dur√©e. "
                "Ajouter une consigne explicite pour expliquer la logique temporelle."
            )
        
        # Recommandations sur l'analyse des performances
        if not analysis_a.get("performance_gap_analyzed"):
            recommendations.append(
                "üü° AM√âLIORATION: L'agent devrait mieux analyser l'√©cart entre "
                "performance actuelle et objectif pour adapter la dur√©e."
            )
        
        # Comparaison entre sc√©narios
        if analysis_a.get("target_time_considered") and not analysis_b.get("target_time_considered"):
            recommendations.append(
                "üü¢ POSITIF: L'agent adapte bien sa r√©ponse selon la pr√©sence d'un temps cible."
            )
        
        return recommendations
    
    async def run_all_tests(self):
        """Ex√©cution de tous les tests"""
        print("üöÄ D√âBUT DES TESTS - Approche objectif-centr√©e")
        print("="*70)
        
        # Initialisation
        if not await self.setup_agent():
            return self.results
        
        # Sc√©nario A : Avec target_time
        prompt_a = self.create_test_prompt_scenario_a()
        result_a = await self.run_scenario("Sc√©nario A", prompt_a)
        result_a["analysis"] = self.analyze_response("Sc√©nario A", result_a["response"])
        self.results["scenarios"]["Sc√©nario A"] = result_a
        
        # Sc√©nario B : Sans target_time  
        prompt_b = self.create_test_prompt_scenario_b()
        result_b = await self.run_scenario("Sc√©nario B", prompt_b)
        result_b["analysis"] = self.analyze_response("Sc√©nario B", result_b["response"])
        self.results["scenarios"]["Sc√©nario B"] = result_b
        
        # G√©n√©ration des recommandations
        self.results["recommendations"] = self.generate_recommendations()
        
        return self.results
    
    def save_results(self, filename="rapport_approche_objectif_centree.json"):
        """Sauvegarde des r√©sultats"""
        filepath = Path(__file__).parent / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"üìÑ Rapport sauvegard√© : {filepath}")
        return filepath

def print_detailed_report(results):
    """Affichage d√©taill√© du rapport"""
    print("\n" + "="*80)
    print("üìã RAPPORT D√âTAILL√â - Tests approche objectif-centr√©e")
    print("="*80)
    
    print(f"\nüïê Timestamp : {results['test_timestamp']}")
    
    for scenario_name, scenario_data in results["scenarios"].items():
        print(f"\n{'üîµ' if scenario_name == 'Sc√©nario A' else 'üü°'} {scenario_name.upper()}")
        print("-" * 50)
        
        analysis = scenario_data["analysis"]
        
        print(f"‚è±Ô∏è  Temps d'ex√©cution : {scenario_data['execution_time']:.2f}s")
        print(f"üîÑ Tentatives : {scenario_data['retries'] + 1}")
        print(f"üìè Dur√©e d√©termin√©e : {'‚úÖ' if analysis['duration_determined'] else '‚ùå'}")
        print(f"üìÖ Semaines planifi√©es : {analysis['duration_weeks']}")
        print(f"üìù Dur√©e justifi√©e : {'‚úÖ' if analysis['duration_justified'] else '‚ùå'}")
        print(f"üìä √âcart performance analys√© : {'‚úÖ' if analysis['performance_gap_analyzed'] else '‚ùå'}")
        print(f"üìã Plan complet g√©n√©r√© : {'‚úÖ' if analysis['complete_plan_generated'] else '‚ùå'}")
        print(f"üéØ Temps cible consid√©r√© : {'‚úÖ' if analysis['target_time_considered'] else '‚ùå'}")
        print(f"‚≠ê Qualit√© : {analysis['recommendations_quality']}")
        
        if scenario_data.get("errors"):
            print(f"‚ùå Erreurs : {len(scenario_data['errors'])}")
            for error in scenario_data["errors"]:
                print(f"   ‚Ä¢ {error}")
        
        # Aper√ßu de la r√©ponse
        response_preview = scenario_data["response"][:300].replace('\n', ' ')
        print(f"üìÑ Aper√ßu r√©ponse : {response_preview}...")
    
    print(f"\nüí° RECOMMANDATIONS ({len(results['recommendations'])})")
    print("-" * 50)
    for i, rec in enumerate(results["recommendations"], 1):
        print(f"{i}. {rec}")
    
    print("\n" + "="*80)

async def main():
    """Fonction principale d'ex√©cution des tests"""
    tester = TestApprocheeObjectifCentree()
    
    try:
        # Ex√©cution des tests
        results = await tester.run_all_tests()
        
        # Affichage du rapport d√©taill√©
        print_detailed_report(results)
        
        # Sauvegarde
        report_file = tester.save_results()
        
        # R√©sum√© final
        total_scenarios = len(results["scenarios"])
        successful_scenarios = sum(
            1 for s in results["scenarios"].values() 
            if not s["response"].startswith("√âCHEC")
        )
        
        print(f"\nüéØ R√âSUM√â FINAL")
        print(f"Sc√©narios test√©s : {total_scenarios}")
        print(f"Sc√©narios r√©ussis : {successful_scenarios}")
        print(f"Recommandations : {len(results['recommendations'])}")
        print(f"Rapport complet : {report_file}")
        
        if successful_scenarios == total_scenarios:
            print("üéâ Tous les tests ont r√©ussi !")
        else:
            print("‚ö†Ô∏è  Certains tests ont √©chou√©, voir le rapport pour d√©tails.")
            
    except Exception as e:
        print(f"‚ùå Erreur lors de l'ex√©cution des tests : {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # V√©rification de l'environnement
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ùå Variable d'environnement OPENAI_API_KEY manquante")
        print("Ajoutez votre cl√© API OpenAI dans le fichier .env")
        sys.exit(1)
    
    # Lancement des tests
    asyncio.run(main())