"""
Script de test RÃ‰EL pour valider l'approche "objectif-centrÃ©e" 
avec la vraie API OpenAI

Version simplifiÃ©e qui charge les variables d'environnement et teste
l'agent rÃ©el avec les nouveaux paramÃ¨tres.
"""

import sys
import os
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path

# Chargement explicit des variables d'environnement
from dotenv import load_dotenv
load_dotenv()

# VÃ©rification de la clÃ© API
if not os.getenv('OPENAI_API_KEY'):
    print("âŒ Variable d'environnement OPENAI_API_KEY manquante")
    print("VÃ©rifiez votre fichier .env")
    sys.exit(1)

# Ajouter le chemin du projet
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.append(project_root)

try:
    from E3_model_IA.scripts.advanced_agent import get_coaching_graph
    from langchain_core.messages import HumanMessage
    print("âœ… Imports rÃ©ussis")
except ImportError as e:
    print(f"âŒ Erreur d'import: {e}")
    print("Tentative avec les dÃ©pendances disponibles...")
    sys.exit(1)

class TestApprocheeObjectifCentreeReel:
    """Tests rÃ©els pour valider l'approche objectif-centrÃ©e"""
    
    def __init__(self):
        self.results = {
            "test_timestamp": datetime.now().isoformat(),
            "mode": "PRODUCTION_API",
            "scenarios": {},
            "analysis": {},
            "recommendations": []
        }
        self.graph = None
        self.user_id = 1
    
    async def setup_agent(self):
        """Initialisation de l'agent IA rÃ©el"""
        print("ğŸ”§ Initialisation de l'agent IA avec API OpenAI...")
        try:
            self.graph = await get_coaching_graph()
            print("âœ… Agent IA initialisÃ© avec succÃ¨s")
            return True
        except Exception as e:
            print(f"âŒ Erreur lors de l'initialisation : {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def create_test_prompt_scenario_a(self):
        """ScÃ©nario A : Test avec target_time="45:00" """
        return f"""
Je suis l'utilisateur {self.user_id}. Je veux tester la nouvelle approche "objectif-centrÃ©e".

PARAMÃˆTRES DE TEST :
- Objectif : Courir un 10k en 45:00 (quarante-cinq minutes)
- Niveau : intermÃ©diaire
- Sessions par semaine : 3
- target_time : "45:00"
- duration_weeks : 0 (IMPORTANT: laisse l'agent dÃ©terminer automatiquement)

INSTRUCTIONS SPÃ‰CIFIQUES POUR L'AGENT :
1. Utilise OBLIGATOIREMENT l'outil get_user_metrics_from_db en premier
2. Recherche des connaissances avec get_training_knowledge 
3. DÃ‰TERMINE AUTOMATIQUEMENT la durÃ©e optimale du plan (PAS seulement 2 semaines !)
4. JUSTIFIE ton choix de durÃ©e en fonction de l'Ã©cart performance actuelle vs objectif 45:00
5. GÃ©nÃ¨re un plan COMPLET sur TOUTE la durÃ©e dÃ©terminÃ©e (minimum 6 semaines)

Mode : plan_generator
L'objectif est de valider que l'agent choisit intelligemment la durÃ©e basÃ©e sur l'objectif.
"""
    
    def create_test_prompt_scenario_b(self):
        """ScÃ©nario B : Test sans target_time (objectif gÃ©nÃ©ral)"""
        return f"""
Je suis l'utilisateur {self.user_id}. Je veux tester l'approche "objectif-centrÃ©e" sans temps cible.

PARAMÃˆTRES DE TEST :
- Objectif : AmÃ©liorer ma performance sur 10k (pas de temps spÃ©cifique)  
- Niveau : intermÃ©diaire
- Sessions par semaine : 3
- target_time : (aucun)
- duration_weeks : 0 (IMPORTANT: laisse l'agent dÃ©terminer automatiquement)

INSTRUCTIONS SPÃ‰CIFIQUES POUR L'AGENT :
1. Utilise OBLIGATOIREMENT l'outil get_user_metrics_from_db en premier
2. Recherche des connaissances avec get_training_knowledge
3. PROPOSE un objectif de temps rÃ©aliste basÃ© sur mes donnÃ©es actuelles
4. DÃ‰TERMINE AUTOMATIQUEMENT une durÃ©e optimale pour le dÃ©veloppement complet
5. JUSTIFIE le choix de durÃ©e pour exploration du potentiel
6. GÃ©nÃ¨re un plan COMPLET sur TOUTE la durÃ©e dÃ©terminÃ©e

Mode : plan_generator
Test de la capacitÃ© de l'agent Ã  proposer durÃ©e ET objectif de faÃ§on autonome.
"""
    
    async def run_scenario(self, scenario_name, prompt, timeout=120):
        """ExÃ©cution d'un scÃ©nario rÃ©el avec timeout"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª EXÃ‰CUTION RÃ‰ELLE - {scenario_name}")
        print(f"{'='*60}")
        
        scenario_result = {
            "prompt": prompt,
            "response": "",
            "execution_time": 0,
            "tool_calls_detected": [],
            "errors": [],
            "timeout_reached": False
        }
        
        try:
            start_time = time.time()
            
            print(f"ğŸ“¤ Envoi du prompt Ã  l'agent OpenAI...")
            print(f"ğŸ“ Prompt (aperÃ§u): {prompt[:150]}...")
            
            # Configuration avec thread unique
            config = {
                "configurable": {
                    "thread_id": f"test-real-{scenario_name.lower().replace(' ', '-')}-{int(time.time())}"
                }
            }
            
            response_parts = []
            tool_calls_detected = []
            
            # Stream avec timeout
            try:
                async with asyncio.timeout(timeout):
                    async for event in self.graph.astream(
                        {"messages": [HumanMessage(content=prompt)], "mode": "plan_generator"},
                        config=config
                    ):
                        for node_name, step in event.items():
                            if "messages" in step and step["messages"]:
                                message = step["messages"][-1]
                                
                                # DÃ©tection des appels d'outils
                                if hasattr(message, 'tool_calls') and message.tool_calls:
                                    for tool_call in message.tool_calls:
                                        tool_name = tool_call.get('name', 'unknown')
                                        tool_calls_detected.append(tool_name)
                                        print(f"ğŸ”§ Outil dÃ©tectÃ© : {tool_name}")
                                
                                # Collecte des rÃ©ponses
                                if hasattr(message, 'content') and message.content:
                                    content = str(message.content)
                                    response_parts.append(content)
                                    print(f"ğŸ“¥ RÃ©ponse partielle ({len(content)} caractÃ¨res)")
                                    
            except asyncio.TimeoutError:
                scenario_result["timeout_reached"] = True
                print(f"â° Timeout atteint aprÃ¨s {timeout}s")
            
            execution_time = time.time() - start_time
            full_response = '\n'.join(response_parts).strip()
            
            if not full_response:
                raise Exception("RÃ©ponse vide de l'agent aprÃ¨s timeout/erreur")
            
            scenario_result.update({
                "response": full_response,
                "execution_time": execution_time,
                "tool_calls_detected": list(set(tool_calls_detected))  # DÃ©duplication
            })
            
            print(f"âœ… ScÃ©nario {scenario_name} exÃ©cutÃ© en {execution_time:.2f}s")
            print(f"ğŸ“Š Outils utilisÃ©s : {scenario_result['tool_calls_detected']}")
            print(f"ğŸ“ RÃ©ponse : {len(full_response)} caractÃ¨res")
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"Erreur: {str(e)}"
            scenario_result["errors"].append(error_msg)
            scenario_result["execution_time"] = execution_time
            print(f"âŒ ScÃ©nario {scenario_name} Ã©chouÃ© aprÃ¨s {execution_time:.2f}s: {error_msg}")
        
        return scenario_result
    
    def analyze_response(self, scenario_name, response, tool_calls):
        """Analyse approfondie de la rÃ©ponse rÃ©elle"""
        analysis = {
            "duration_determined": False,
            "duration_weeks": 0,
            "duration_justified": False,
            "performance_gap_analyzed": False,
            "complete_plan_generated": False,
            "tools_used_correctly": False,
            "target_time_addressed": False,
            "objective_proposed": False,
            "quality_score": 0
        }
        
        response_lower = response.lower()
        
        # 1. Analyse des outils utilisÃ©s
        expected_tools = ["get_user_metrics_from_db", "get_training_knowledge"]
        tools_used = len([t for t in expected_tools if t in tool_calls])
        analysis["tools_used_correctly"] = tools_used >= 1
        
        # 2. DÃ©tection durÃ©e dÃ©terminÃ©e
        duration_keywords = [
            "durÃ©e dÃ©terminÃ©e", "semaines", "plan de", "programme de", 
            "durÃ©e optimale", "durÃ©e recommandÃ©e"
        ]
        analysis["duration_determined"] = any(kw in response_lower for kw in duration_keywords)
        
        # 3. Extraction nombre de semaines
        import re
        week_patterns = [
            r'(\d+)\s*semaines?',
            r'plan de\s*(\d+)',
            r'programme de\s*(\d+)',  
            r'durÃ©e.*?(\d+).*?semaines?',
            r'##\s*semaine\s*(\d+)'
        ]
        
        week_numbers = []
        for pattern in week_patterns:
            matches = re.findall(pattern, response_lower)
            week_numbers.extend([int(m) for m in matches if int(m) <= 20])  # Semaines rÃ©alistes
        
        if week_numbers:
            analysis["duration_weeks"] = max(week_numbers)
        
        # 4. Justification prÃ©sente
        justification_keywords = [
            "justification", "parce que", "car", "en raison", "afin de",
            "nÃ©cessite", "permet", "Ã©cart", "objectif", "progression"
        ]
        analysis["duration_justified"] = any(kw in response_lower for kw in justification_keywords)
        
        # 5. Analyse Ã©cart performance
        gap_keywords = [
            "Ã©cart", "diffÃ©rence", "amÃ©lioration", "progression", "actuel",
            "performance", "vitesse", "temps", "objectif"
        ]
        analysis["performance_gap_analyzed"] = any(kw in response_lower for kw in gap_keywords)
        
        # 6. Plan complet gÃ©nÃ©rÃ©
        plan_keywords = [
            "semaine 1", "semaine 2", "tableau", "jour", "sÃ©ance",
            "lundi", "mardi", "entraÃ®nement", "plan hebdomadaire"
        ]
        plan_count = sum(1 for kw in plan_keywords if kw in response_lower)
        analysis["complete_plan_generated"] = plan_count >= 3
        
        # 7. ScÃ©nario A : target_time pris en compte
        if scenario_name == "ScÃ©nario A":
            time_keywords = ["45:00", "quarante-cinq", "45 minutes", "temps objectif", "temps cible"]
            analysis["target_time_addressed"] = any(kw in response_lower for kw in time_keywords)
        
        # 8. ScÃ©nario B : objectif proposÃ©
        if scenario_name == "ScÃ©nario B":
            proposal_keywords = [
                "objectif proposÃ©", "objectif recommandÃ©", "temps suggÃ©rÃ©",
                "potentiel", "amÃ©lioration de", "passer de"
            ]
            analysis["objective_proposed"] = any(kw in response_lower for kw in proposal_keywords)
        
        # 9. Score qualitÃ©
        score = 0
        if analysis["tools_used_correctly"]: score += 2
        if analysis["duration_determined"]: score += 2  
        if analysis["duration_weeks"] >= 6: score += 2
        if analysis["duration_justified"]: score += 1
        if analysis["performance_gap_analyzed"]: score += 1
        if analysis["complete_plan_generated"]: score += 2
        
        analysis["quality_score"] = score
        return analysis
    
    def generate_recommendations(self):
        """Recommandations basÃ©es sur les tests rÃ©els"""
        recommendations = []
        
        scenario_a = self.results["scenarios"].get("ScÃ©nario A", {})
        scenario_b = self.results["scenarios"].get("ScÃ©nario B", {}) 
        
        analysis_a = scenario_a.get("analysis", {})
        analysis_b = scenario_b.get("analysis", {})
        
        # Ã‰valuation globale
        if analysis_a.get("quality_score", 0) >= 8 and analysis_b.get("quality_score", 0) >= 8:
            recommendations.append(
                "ğŸŸ¢ SUCCÃˆS TOTAL: L'approche objectif-centrÃ©e fonctionne parfaitement "
                "avec l'API OpenAI rÃ©elle. DÃ©ploiement recommandÃ©."
            )
        elif analysis_a.get("quality_score", 0) >= 6 or analysis_b.get("quality_score", 0) >= 6:
            recommendations.append(
                "ğŸŸ¡ SUCCÃˆS PARTIEL: L'approche fonctionne mais nÃ©cessite des ajustements "
                "dans les prompts systÃ¨me avant dÃ©ploiement."
            )
        else:
            recommendations.append(
                "ğŸ”´ Ã‰CHEC: L'approche nÃ©cessite des modifications importantes "
                "avant d'Ãªtre utilisable en production."
            )
        
        # Recommandations spÃ©cifiques
        duration_a = analysis_a.get("duration_weeks", 0)
        duration_b = analysis_b.get("duration_weeks", 0)
        
        if duration_a >= 6 and duration_b >= 6:
            recommendations.append(f"âœ… DurÃ©es appropriÃ©es gÃ©nÃ©rÃ©es: {duration_a} et {duration_b} semaines")
        else:
            recommendations.append(f"âŒ DurÃ©es insuffisantes: {duration_a} et {duration_b} semaines")
        
        if analysis_a.get("tools_used_correctly") and analysis_b.get("tools_used_correctly"):
            recommendations.append("âœ… Outils utilisÃ©s correctement dans les deux scÃ©narios")
        else:
            recommendations.append("âŒ ProblÃ¨me d'utilisation des outils dÃ©tectÃ©")
        
        return recommendations
    
    async def run_all_tests(self):
        """ExÃ©cution complÃ¨te des tests rÃ©els"""
        print("ğŸš€ DÃ‰BUT DES TESTS RÃ‰ELS - Approche objectif-centrÃ©e")
        print("="*70)
        print("ğŸŒ Tests avec API OpenAI authentique")
        
        # Initialisation
        if not await self.setup_agent():
            return self.results
        
        # ScÃ©nario A - Avec target_time
        prompt_a = self.create_test_prompt_scenario_a()
        result_a = await self.run_scenario("ScÃ©nario A", prompt_a)
        result_a["analysis"] = self.analyze_response("ScÃ©nario A", result_a["response"], result_a["tool_calls_detected"])
        self.results["scenarios"]["ScÃ©nario A"] = result_a
        
        # ScÃ©nario B - Sans target_time
        prompt_b = self.create_test_prompt_scenario_b()
        result_b = await self.run_scenario("ScÃ©nario B", prompt_b)
        result_b["analysis"] = self.analyze_response("ScÃ©nario B", result_b["response"], result_b["tool_calls_detected"]) 
        self.results["scenarios"]["ScÃ©nario B"] = result_b
        
        # Recommandations
        self.results["recommendations"] = self.generate_recommendations()
        
        return self.results
    
    def save_results(self, filename="rapport_reel_approche_objectif_centree.json"):
        """Sauvegarde des rÃ©sultats rÃ©els"""
        filepath = Path(__file__).parent / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        print(f"ğŸ“„ Rapport rÃ©el sauvegardÃ© : {filepath}")
        return filepath

def print_real_report(results):
    """Affichage du rapport des tests rÃ©els"""
    print("\n" + "="*80)
    print("ğŸ“‹ RAPPORT TESTS RÃ‰ELS - Approche objectif-centrÃ©e")  
    print("="*80)
    
    print(f"ğŸ• Timestamp : {results['test_timestamp']}")
    print(f"ğŸŒ Mode : {results['mode']}")
    
    for scenario_name, scenario_data in results["scenarios"].items():
        print(f"\n{'ğŸ”µ' if scenario_name == 'ScÃ©nario A' else 'ğŸŸ¡'} {scenario_name.upper()}")
        print("-" * 50)
        
        analysis = scenario_data.get("analysis", {})
        
        print(f"â±ï¸  Temps d'exÃ©cution : {scenario_data.get('execution_time', 0):.2f}s")
        print(f"ğŸ”§ Outils dÃ©tectÃ©s : {scenario_data.get('tool_calls_detected', [])}")
        print(f"ğŸ“ DurÃ©e dÃ©terminÃ©e : {'âœ…' if analysis.get('duration_determined') else 'âŒ'}")
        print(f"ğŸ“… Semaines planifiÃ©es : {analysis.get('duration_weeks', 0)}")
        print(f"ğŸ“ DurÃ©e justifiÃ©e : {'âœ…' if analysis.get('duration_justified') else 'âŒ'}")
        print(f"ğŸ“Š Ã‰cart analysÃ© : {'âœ…' if analysis.get('performance_gap_analyzed') else 'âŒ'}")
        print(f"ğŸ“‹ Plan complet : {'âœ…' if analysis.get('complete_plan_generated') else 'âŒ'}")
        print(f"ğŸ¯ Score qualitÃ© : {analysis.get('quality_score', 0)}/10")
        
        if scenario_data.get("errors"):
            print(f"âŒ Erreurs : {len(scenario_data['errors'])}")
            for error in scenario_data["errors"][:2]:  # Limite affichage erreurs
                print(f"   â€¢ {error}")
    
    print(f"\nğŸ’¡ RECOMMANDATIONS FINALES ({len(results.get('recommendations', []))})")
    print("-" * 50)
    for i, rec in enumerate(results.get("recommendations", []), 1):
        print(f"{i}. {rec}")
    
    print("\n" + "="*80)

async def main():
    """Fonction principale des tests rÃ©els"""
    print("ğŸ”‘ VÃ©rification de l'environnement...")
    print(f"âœ… OPENAI_API_KEY prÃ©sente : {bool(os.getenv('OPENAI_API_KEY'))}")
    
    tester = TestApprocheeObjectifCentreeReel()
    
    try:
        # ExÃ©cution
        results = await tester.run_all_tests()
        
        # Rapport
        print_real_report(results)
        
        # Sauvegarde
        report_file = tester.save_results()
        
        # RÃ©sumÃ©
        successful_scenarios = len([s for s in results["scenarios"].values() 
                                   if not s.get("errors") and s.get("response")])
        total_scenarios = len(results["scenarios"])
        
        print(f"\nğŸ¯ RÃ‰SUMÃ‰ FINAL")
        print(f"ScÃ©narios testÃ©s : {total_scenarios}")
        print(f"ScÃ©narios rÃ©ussis : {successful_scenarios}")
        print(f"Recommandations : {len(results['recommendations'])}")
        print(f"Rapport : {report_file}")
        
        if successful_scenarios == total_scenarios:
            print("ğŸ‰ Tous les tests rÃ©els ont rÃ©ussi !")
        else:
            print("âš ï¸  Certains tests ont Ã©chouÃ© - voir rapport")
        
    except Exception as e:
        print(f"âŒ Erreur critique : {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())