"""
Script de test R√âEL pour valider l'approche "objectif-centr√©e" 
avec la vraie API OpenAI

Version simplifi√©e qui charge les variables d'environnement et teste
l'agent r√©el avec les nouveaux param√®tres.
"""

import sys
import os
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path

# Chargement explicit des variables d'environnement
from dotenv import load_dotenv
load_dotenv()

# V√©rification de la cl√© API
if not os.getenv('OPENAI_API_KEY'):
    print("‚ùå Variable d'environnement OPENAI_API_KEY manquante")
    print("V√©rifiez votre fichier .env")
    sys.exit(1)

# Ajouter le chemin du projet
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.append(project_root)

try:
    from E3_model_IA.scripts.advanced_agent import get_coaching_graph
    from langchain_core.messages import HumanMessage
    print("‚úÖ Imports r√©ussis")
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    print("Tentative avec les d√©pendances disponibles...")
    sys.exit(1)

class TestApprocheeObjectifCentreeReel:
    """Tests r√©els pour valider l'approche objectif-centr√©e"""
    
    def __init__(self):
        self.results = {
            "test_timestamp": datetime.now().isoformat(),
            "mode": "PRODUCTION_API",
            "scenarios": {},
            "analysis": {},
            "recommendations": []
        }
        self.graph = None
        self.user_id = 1
    
    async def setup_agent(self):
        """Initialisation de l'agent IA r√©el"""
        print("üîß Initialisation de l'agent IA avec API OpenAI...")
        try:
            self.graph = await get_coaching_graph()
            print("‚úÖ Agent IA initialis√© avec succ√®s")
            return True
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation : {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def create_test_prompt_scenario_a(self):
        """Sc√©nario A : Test avec target_time="45:00" """
        return f"""
Je suis l'utilisateur {self.user_id}. Je veux tester la nouvelle approche "objectif-centr√©e".

PARAM√àTRES DE TEST :
- Objectif : Courir un 10k en 45:00 (quarante-cinq minutes)
- Niveau : interm√©diaire
- Sessions par semaine : 3
- target_time : "45:00"
- duration_weeks : 0 (IMPORTANT: laisse l'agent d√©terminer automatiquement)

INSTRUCTIONS SP√âCIFIQUES POUR L'AGENT :
1. Utilise OBLIGATOIREMENT l'outil get_user_metrics_from_db en premier
2. Recherche des connaissances avec get_training_knowledge 
3. D√âTERMINE AUTOMATIQUEMENT la dur√©e optimale du plan (PAS seulement 2 semaines !)
4. JUSTIFIE ton choix de dur√©e en fonction de l'√©cart performance actuelle vs objectif 45:00
5. G√©n√®re un plan COMPLET sur TOUTE la dur√©e d√©termin√©e (minimum 6 semaines)

Mode : plan_generator
L'objectif est de valider que l'agent choisit intelligemment la dur√©e bas√©e sur l'objectif.
"""
    
    def create_test_prompt_scenario_b(self):
        """Sc√©nario B : Test sans target_time (objectif g√©n√©ral)"""
        return f"""
Je suis l'utilisateur {self.user_id}. Je veux tester l'approche "objectif-centr√©e" sans temps cible.

PARAM√àTRES DE TEST :
- Objectif : Am√©liorer ma performance sur 10k (pas de temps sp√©cifique)  
- Niveau : interm√©diaire
- Sessions par semaine : 3
- target_time : (aucun)
- duration_weeks : 0 (IMPORTANT: laisse l'agent d√©terminer automatiquement)

INSTRUCTIONS SP√âCIFIQUES POUR L'AGENT :
1. Utilise OBLIGATOIREMENT l'outil get_user_metrics_from_db en premier
2. Recherche des connaissances avec get_training_knowledge
3. PROPOSE un objectif de temps r√©aliste bas√© sur mes donn√©es actuelles
4. D√âTERMINE AUTOMATIQUEMENT une dur√©e optimale pour le d√©veloppement complet
5. JUSTIFIE le choix de dur√©e pour exploration du potentiel
6. G√©n√®re un plan COMPLET sur TOUTE la dur√©e d√©termin√©e

Mode : plan_generator
Test de la capacit√© de l'agent √† proposer dur√©e ET objectif de fa√ßon autonome.
"""
    
    async def run_scenario(self, scenario_name, prompt, timeout=120):
        """Ex√©cution d'un sc√©nario r√©el avec timeout"""
        print(f"\n{'='*60}")
        print(f"üß™ EX√âCUTION R√âELLE - {scenario_name}")
        print(f"{'='*60}")
        
        scenario_result = {
            "prompt": prompt,
            "response": "",
            "execution_time": 0,
            "tool_calls_detected": [],
            "errors": [],
            "timeout_reached": False
        }
        
        try:
            start_time = time.time()
            
            print(f"üì§ Envoi du prompt √† l'agent OpenAI...")
            print(f"üìù Prompt (aper√ßu): {prompt[:150]}...")
            
            # Configuration avec thread unique
            config = {
                "configurable": {
                    "thread_id": f"test-real-{scenario_name.lower().replace(' ', '-')}-{int(time.time())}"
                }
            }
            
            response_parts = []
            tool_calls_detected = []
            
            # Stream avec timeout
            try:
                async with asyncio.timeout(timeout):
                    async for event in self.graph.astream(
                        {"messages": [HumanMessage(content=prompt)], "mode": "plan_generator"},
                        config=config
                    ):
                        for node_name, step in event.items():
                            if "messages" in step and step["messages"]:
                                message = step["messages"][-1]
                                
                                # D√©tection des appels d'outils
                                if hasattr(message, 'tool_calls') and message.tool_calls:
                                    for tool_call in message.tool_calls:
                                        tool_name = tool_call.get('name', 'unknown')
                                        tool_calls_detected.append(tool_name)
                                        print(f"üîß Outil d√©tect√© : {tool_name}")
                                
                                # Collecte des r√©ponses
                                if hasattr(message, 'content') and message.content:
                                    content = str(message.content)
                                    response_parts.append(content)
                                    print(f"üì• R√©ponse partielle ({len(content)} caract√®res)")
                                    
            except asyncio.TimeoutError:
                scenario_result["timeout_reached"] = True
                print(f"‚è∞ Timeout atteint apr√®s {timeout}s")
            
            execution_time = time.time() - start_time
            full_response = '\n'.join(response_parts).strip()
            
            if not full_response:
                raise Exception("R√©ponse vide de l'agent apr√®s timeout/erreur")
            
            scenario_result.update({
                "response": full_response,
                "execution_time": execution_time,
                "tool_calls_detected": list(set(tool_calls_detected))  # D√©duplication
            })
            
            print(f"‚úÖ Sc√©nario {scenario_name} ex√©cut√© en {execution_time:.2f}s")
            print(f"üìä Outils utilis√©s : {scenario_result['tool_calls_detected']}")
            print(f"üìè R√©ponse : {len(full_response)} caract√®res")
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"Erreur: {str(e)}"
            scenario_result["errors"].append(error_msg)
            scenario_result["execution_time"] = execution_time
            print(f"‚ùå Sc√©nario {scenario_name} √©chou√© apr√®s {execution_time:.2f}s: {error_msg}")
        
        return scenario_result
    
    def analyze_response(self, scenario_name, response, tool_calls):
        """Analyse approfondie de la r√©ponse r√©elle"""
        analysis = {
            "duration_determined": False,
            "duration_weeks": 0,
            "duration_justified": False,
            "performance_gap_analyzed": False,
            "complete_plan_generated": False,
            "tools_used_correctly": False,
            "target_time_addressed": False,
            "objective_proposed": False,
            "quality_score": 0
        }
        
        response_lower = response.lower()
        
        # 1. Analyse des outils utilis√©s
        expected_tools = ["get_user_metrics_from_db", "get_training_knowledge"]
        tools_used = len([t for t in expected_tools if t in tool_calls])
        analysis["tools_used_correctly"] = tools_used >= 1
        
        # 2. D√©tection dur√©e d√©termin√©e
        duration_keywords = [
            "dur√©e d√©termin√©e", "semaines", "plan de", "programme de", 
            "dur√©e optimale", "dur√©e recommand√©e"
        ]
        analysis["duration_determined"] = any(kw in response_lower for kw in duration_keywords)
        
        # 3. Extraction nombre de semaines
        import re
        week_patterns = [
            r'(\d+)\s*semaines?',
            r'plan de\s*(\d+)',
            r'programme de\s*(\d+)',  
            r'dur√©e.*?(\d+).*?semaines?',
            r'##\s*semaine\s*(\d+)'
        ]
        
        week_numbers = []
        for pattern in week_patterns:
            matches = re.findall(pattern, response_lower)
            week_numbers.extend([int(m) for m in matches if int(m) <= 20])  # Semaines r√©alistes
        
        if week_numbers:
            analysis["duration_weeks"] = max(week_numbers)
        
        # 4. Justification pr√©sente
        justification_keywords = [
            "justification", "parce que", "car", "en raison", "afin de",
            "n√©cessite", "permet", "√©cart", "objectif", "progression"
        ]
        analysis["duration_justified"] = any(kw in response_lower for kw in justification_keywords)
        
        # 5. Analyse √©cart performance
        gap_keywords = [
            "√©cart", "diff√©rence", "am√©lioration", "progression", "actuel",
            "performance", "vitesse", "temps", "objectif"
        ]
        analysis["performance_gap_analyzed"] = any(kw in response_lower for kw in gap_keywords)
        
        # 6. Plan complet g√©n√©r√©
        plan_keywords = [
            "semaine 1", "semaine 2", "tableau", "jour", "s√©ance",
            "lundi", "mardi", "entra√Ænement", "plan hebdomadaire"
        ]
        plan_count = sum(1 for kw in plan_keywords if kw in response_lower)
        analysis["complete_plan_generated"] = plan_count >= 3
        
        # 7. Sc√©nario A : target_time pris en compte
        if scenario_name == "Sc√©nario A":
            time_keywords = ["45:00", "quarante-cinq", "45 minutes", "temps objectif", "temps cible"]
            analysis["target_time_addressed"] = any(kw in response_lower for kw in time_keywords)
        
        # 8. Sc√©nario B : objectif propos√©
        if scenario_name == "Sc√©nario B":
            proposal_keywords = [
                "objectif propos√©", "objectif recommand√©", "temps sugg√©r√©",
                "potentiel", "am√©lioration de", "passer de"
            ]
            analysis["objective_proposed"] = any(kw in response_lower for kw in proposal_keywords)
        
        # 9. Score qualit√©
        score = 0
        if analysis["tools_used_correctly"]: score += 2
        if analysis["duration_determined"]: score += 2  
        if analysis["duration_weeks"] >= 6: score += 2
        if analysis["duration_justified"]: score += 1
        if analysis["performance_gap_analyzed"]: score += 1
        if analysis["complete_plan_generated"]: score += 2
        
        analysis["quality_score"] = score
        return analysis
    
    def generate_recommendations(self):
        """Recommandations bas√©es sur les tests r√©els"""
        recommendations = []
        
        scenario_a = self.results["scenarios"].get("Sc√©nario A", {})
        scenario_b = self.results["scenarios"].get("Sc√©nario B", {}) 
        
        analysis_a = scenario_a.get("analysis", {})
        analysis_b = scenario_b.get("analysis", {})
        
        # √âvaluation globale
        if analysis_a.get("quality_score", 0) >= 8 and analysis_b.get("quality_score", 0) >= 8:
            recommendations.append(
                "üü¢ SUCC√àS TOTAL: L'approche objectif-centr√©e fonctionne parfaitement "
                "avec l'API OpenAI r√©elle. D√©ploiement recommand√©."
            )
        elif analysis_a.get("quality_score", 0) >= 6 or analysis_b.get("quality_score", 0) >= 6:
            recommendations.append(
                "üü° SUCC√àS PARTIEL: L'approche fonctionne mais n√©cessite des ajustements "
                "dans les prompts syst√®me avant d√©ploiement."
            )
        else:
            recommendations.append(
                "üî¥ √âCHEC: L'approche n√©cessite des modifications importantes "
                "avant d'√™tre utilisable en production."
            )
        
        # Recommandations sp√©cifiques
        duration_a = analysis_a.get("duration_weeks", 0)
        duration_b = analysis_b.get("duration_weeks", 0)
        
        if duration_a >= 6 and duration_b >= 6:
            recommendations.append(f"‚úÖ Dur√©es appropri√©es g√©n√©r√©es: {duration_a} et {duration_b} semaines")
        else:
            recommendations.append(f"‚ùå Dur√©es insuffisantes: {duration_a} et {duration_b} semaines")
        
        if analysis_a.get("tools_used_correctly") and analysis_b.get("tools_used_correctly"):
            recommendations.append("‚úÖ Outils utilis√©s correctement dans les deux sc√©narios")
        else:
            recommendations.append("‚ùå Probl√®me d'utilisation des outils d√©tect√©")
        
        return recommendations
    
    async def run_all_tests(self):
        """Ex√©cution compl√®te des tests r√©els"""
        print("üöÄ D√âBUT DES TESTS R√âELS - Approche objectif-centr√©e")
        print("="*70)
        print("üåê Tests avec API OpenAI authentique")
        
        # Initialisation
        if not await self.setup_agent():
            return self.results
        
        # Sc√©nario A - Avec target_time
        prompt_a = self.create_test_prompt_scenario_a()
        result_a = await self.run_scenario("Sc√©nario A", prompt_a)
        result_a["analysis"] = self.analyze_response("Sc√©nario A", result_a["response"], result_a["tool_calls_detected"])
        self.results["scenarios"]["Sc√©nario A"] = result_a
        
        # Sc√©nario B - Sans target_time
        prompt_b = self.create_test_prompt_scenario_b()
        result_b = await self.run_scenario("Sc√©nario B", prompt_b)
        result_b["analysis"] = self.analyze_response("Sc√©nario B", result_b["response"], result_b["tool_calls_detected"]) 
        self.results["scenarios"]["Sc√©nario B"] = result_b
        
        # Recommandations
        self.results["recommendations"] = self.generate_recommendations()
        
        return self.results
    
    def save_results(self, filename="rapport_reel_approche_objectif_centree.json"):
        """Sauvegarde des r√©sultats r√©els"""
        filepath = Path(__file__).parent / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        print(f"üìÑ Rapport r√©el sauvegard√© : {filepath}")
        return filepath

def print_real_report(results):
    """Affichage du rapport des tests r√©els"""
    print("\n" + "="*80)
    print("üìã RAPPORT TESTS R√âELS - Approche objectif-centr√©e")  
    print("="*80)
    
    print(f"üïê Timestamp : {results['test_timestamp']}")
    print(f"üåê Mode : {results['mode']}")
    
    for scenario_name, scenario_data in results["scenarios"].items():
        print(f"\n{'üîµ' if scenario_name == 'Sc√©nario A' else 'üü°'} {scenario_name.upper()}")
        print("-" * 50)
        
        analysis = scenario_data.get("analysis", {})
        
        print(f"‚è±Ô∏è  Temps d'ex√©cution : {scenario_data.get('execution_time', 0):.2f}s")
        print(f"üîß Outils d√©tect√©s : {scenario_data.get('tool_calls_detected', [])}")
        print(f"üìè Dur√©e d√©termin√©e : {'‚úÖ' if analysis.get('duration_determined') else '‚ùå'}")
        print(f"üìÖ Semaines planifi√©es : {analysis.get('duration_weeks', 0)}")
        print(f"üìù Dur√©e justifi√©e : {'‚úÖ' if analysis.get('duration_justified') else '‚ùå'}")
        print(f"üìä √âcart analys√© : {'‚úÖ' if analysis.get('performance_gap_analyzed') else '‚ùå'}")
        print(f"üìã Plan complet : {'‚úÖ' if analysis.get('complete_plan_generated') else '‚ùå'}")
        print(f"üéØ Score qualit√© : {analysis.get('quality_score', 0)}/10")
        
        if scenario_data.get("errors"):
            print(f"‚ùå Erreurs : {len(scenario_data['errors'])}")
            for error in scenario_data["errors"][:2]:  # Limite affichage erreurs
                print(f"   ‚Ä¢ {error}")
    
    print(f"\nüí° RECOMMANDATIONS FINALES ({len(results.get('recommendations', []))})")
    print("-" * 50)
    for i, rec in enumerate(results.get("recommendations", []), 1):
        print(f"{i}. {rec}")
    
    print("\n" + "="*80)

async def main():
    """Fonction principale des tests r√©els"""
    print("üîë V√©rification de l'environnement...")
    print(f"‚úÖ OPENAI_API_KEY pr√©sente : {bool(os.getenv('OPENAI_API_KEY'))}")
    
    tester = TestApprocheeObjectifCentreeReel()
    
    try:
        # Ex√©cution
        results = await tester.run_all_tests()
        
        # Rapport
        print_real_report(results)
        
        # Sauvegarde
        report_file = tester.save_results()
        
        # R√©sum√©
        successful_scenarios = len([s for s in results["scenarios"].values() 
                                   if not s.get("errors") and s.get("response")])
        total_scenarios = len(results["scenarios"])
        
        print(f"\nüéØ R√âSUM√â FINAL")
        print(f"Sc√©narios test√©s : {total_scenarios}")
        print(f"Sc√©narios r√©ussis : {successful_scenarios}")
        print(f"Recommandations : {len(results['recommendations'])}")
        print(f"Rapport : {report_file}")
        
        if successful_scenarios == total_scenarios:
            print("üéâ Tous les tests r√©els ont r√©ussi !")
        else:
            print("‚ö†Ô∏è  Certains tests ont √©chou√© - voir rapport")
        
    except Exception as e:
        print(f"‚ùå Erreur critique : {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())